# AvePoint ML Developer Playbook

> **"Do Not Repeat" (DNR)**: This document captures hard-earned learnings from early use cases (UC-01) to ensure smooth, reproducible deployments for future workloads.

## 1. Naming Conventions

Strict adherence to naming is critical for automation scripts to function correctly.

| Entity | Pattern | Example | Notes |
| :--- | :--- | :--- | :--- |
| **Git Repository** | `kebab-case` | `avepoint-ml` | The root container for all use cases. |
| **Use Case Directory** | `uc{NN}-{name}` | `uc01-permission-anomaly` | Must start with `uc` followed by 2 digits. |
| **Notebook Files** | `{NN}_{name}.py` | `00_generate_data.py` | Must start with 2 digits for sorting. Use underscores. |
| **Fabric Display Name** | `UC{NN}_{Title_Case}` | `UC01_00_Generate_Data` | Auto-generated by deployment script. |
| **Fabric Pipeline** | `UC{NN}_Automated_Pipeline` | `UC01_Automated_Pipeline` | The single orchestration artifact per UC. |
| **Fabric Lakehouse** | `UC{NN}_{Name}_Lakehouse` | `UC01_PermissionAnomaly_Lakehouse` | One lakehouse per UC. |

## 2. Microsoft Fabric Deployment

### Automation Strategy
We use **programmatic deployment** via the Fabric REST API, bypassing the UI.
- **Script**: Each UC has a `scripts/deploy_to_fabric.py`.
- **Logic**:
    1.  Authenticates via Service Principal.
    2.  Creates/Gets Lakehouse.
    3.  Scans `notebooks/` folder.
    4.  Converts `.py` -> `.ipynb` and uploads.
    5.  Creates/Updates a **Data Pipeline** chaining all notebooks.
    6.  **Triggers** the pipeline immediately for verification.

### "Task Flows" vs. "Data Pipelines"
> [!WARNING]
> **Do not attempt to automate "Task Flows".**
> The "Task Flow" visual (the green boxes in the Workspace UI) is a **UI-only feature** with no public API.
> **Solution**: We rely on **Data Pipelines** for functional orchestration. Use the "Monitoring Hub" in Fabric or the pipeline run view to visualize progress.

## 3. Secret Management

**NEVER commit credentials, Tenant IDs, or Client Secrets to Git.**

### Local Development
1.  Use `python-dotenv`.
2.  Create a `.env` file in the Use Case directory (e.g., `uc01-permission-anomaly/.env`).
3.  Add `.env` to `.gitignore` (Global ignore is configured).
4.  Use `.env.template` to share the required keys without values.

**Required Keys**:
```bash
AZURE_CLIENT_SECRET=your_secret_here
```

### Script Usage
```python
from dotenv import load_dotenv
load_dotenv()
CLIENT_SECRET = os.getenv("AZURE_CLIENT_SECRET")
```

## 4. Notebook Development Guidelines

- **Format**: Write code in standard `.py` files.
- **Location**: Store in `notebooks/` folder within the Use Case directory.
- **Ordering**: Prefix filenames with `00_`, `01_`, `02_` to define execution order. The deployment script uses this to build the pipeline dependency graph.
- **Idempotency**: Ensure notebooks can be re-run without failure (e.g., use `CREATE OR REPLACE` logic).

### 5. API "Gotchas"
- **Async Creation**: Creating a Notebook returns `202 Accepted`. You **must** poll or wait before attempting to read its ID for use in a Pipeline. The deployment script includes a retry loop for this.
- **Update Definition**: When updating an item, the payload **must** be wrapped in `{"definition": ...}`.

## 6. Workflow for New Use Cases

1.  **Scaffold**: Run `python scripts/new_usecase.py` (Coming Soon).
2.  **Develop**: Add `.py` scripts to `notebooks/`.
3.  **Config**: Update `deploy.py` constants (Workspace ID) if changing environments.
4.  **Deploy**: Run `uv run python scripts/deploy_to_fabric.py`.
5.  **Verify**: Check console output for Job Instance ID.
